# Awesome Natural Language Processing
I share information related to the NLP what I interested in.

- modified : 2023-12-03



# Overview

- [Neural Machine Translation](#Neural-Machine-Translation)
- [Attention](#Attention)
- [Position Embedding](#Position-Embedding)
- [Survey](#Survey)
- [Models](#Models)
- [Others](#Others)



# Papers

### Neural Machine Translation

- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf) (ICML'14)

- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf) (EMNLP'15)


### Attention

- [Sparse is Enough in Scaling Transformers](https://openreview.net/pdf?id=-b5OSCydOMe) (NeurIPS'21)



### Position Embedding

- [Improve Transformer Models with Better Relative Position Embeddings](https://arxiv.org/pdf/2009.13658.pdf) (EMNLP'20)

- [Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/pdf/2108.12409.pdf) (ICLR'22)

- [Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf) (Neurocomputing'23)



### Survey

- [A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges](https://arxiv.org/pdf/2203.01054.pdf)

- [AMMUS: A Survey of Transformer-based Pretrained Models in Natural Language Processing](https://arxiv.org/pdf/2108.05542.pdf)



### Models

- Mamba (ICLR'24): [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)

- LittleBird (EMNLP'22): [LittleBird: Efficient Faster & Longer Transformer for Question Answering](https://aclanthology.org/2022.emnlp-main.352.pdf)

- ALiBi (ICLR'22): [Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/pdf/2108.12409.pdf)

- IA3 (NIPS'22): [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/pdf/2205.05638.pdf)

- LoRA (ICLR'22): [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)

- Roformer (Neurocomputing'21): [Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)

- SimCSE (EMNLP'21): [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/pdf/2104.08821.pdf)

- GPT-3 (NIPS'20): [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)

- ELECTRA (ICLR'20): [Electra: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/pdf/2003.10555.pdf)

- BigBird (NIPS'20): [Big bird: Transformers for Longer Sequences](https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf)

- Reformer (ICLR'20): [Reformer: The efficient transformer](https://arxiv.org/pdf/2001.04451.pdf)

- T5 (JMLR'20): [Exploring the limits of transfer learning with a unified text-to-text transformer](https://dl.acm.org/doi/pdf/10.5555/3455716.3455856)

- BERT (NAACL'19): [Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

- XLNet (NIPS'19): [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

- GPT-2 (2019): [Language Models are Unsupervised Multitask Learners](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf)

- XLM (2019): [Cross-lingual Language Model Pretraining](https://arxiv.org/pdf/1901.07291.pdf)

- BART (2019): [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)

- BERT-E2E-ABSA (2019): [Exploiting BERT for End-to-End Aspect-based Sentiment Analysis](https://arxiv.org/pdf/1910.00883.pdf)

- RoBERTa (2019): [A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)

- CTRL (2019): [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/pdf/1909.05858.pdf)

- ALBERT (2019): [Albert: A Lite Bert for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)

- SBERT (2019): [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf)

- DistillBERT (2019): [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf)

- ELMo (2018): [ELMo: Deep Contextualizaed word representations](https://arxiv.org/pdf/1802.05365.pdf)

- GPT (2018): [Improving Language Understandingby Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

- Transformer (NIPS'17) : [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

- Seq2Seq (ICML'14): [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)

- GloVe (EMNLP'14): [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162.pdf)

- Word2Vec (NIPS'13): [Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)

- COALS (ACM'06): [An improved model of semantic similarity based on lexical co-occurrence](https://www.cnbc.cmu.edu/~plaut/papers/pdf/RohdeGonnermanPlautSUB-CogSci.COALS.pdf)

- NNLM (NIPS'00) [A Neural Probabilistic Language Model](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)



### Others

- [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://arxiv.org/pdf/1901.11196.pdf) (EMNLP-IJCNLP'19) `Data Augmentation`
- [Visualizing and Understanding Recurrent Networks](https://arxiv.org/pdf/1506.02078.pdf) (ICLR'16) `eXplainable AI`
- [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/pdf/2002.06305.pdf) (ArXiv'20) `Fine-Tuning`
- [Learning to Memorize Entailment and Discourse Relations for Persona-Consistent]() (AAAI'23) `Persona`
